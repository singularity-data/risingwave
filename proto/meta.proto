syntax = "proto3";

package meta;

import "catalog.proto";
import "common.proto";
import "hummock.proto";
import "stream_plan.proto";
import "user.proto";

option optimize_for = SPEED;

message HeartbeatRequest {
  uint32 node_id = 1;
}

message HeartbeatResponse {
  common.Status status = 1;
}

service HeartbeatService {
  rpc Heartbeat(HeartbeatRequest) returns (HeartbeatResponse);
}

// Fragments of a Materialized View
message TableFragments {
  // Current state of actor
  enum ActorState {
    UNSPECIFIED = 0;
    // Initial state after creation
    INACTIVE = 1;
    // Running normally
    RUNNING = 2;
  }
  // Runtime information of an actor
  message ActorStatus {
    // Current on which parallel unit
    common.ParallelUnit parallel_unit = 1;
    // Current state
    ActorState state = 2;
  }
  message Fragment {
    enum FragmentDistributionType {
      UNSPECIFIED = 0;
      SINGLE = 1;
      HASH = 2;
    }
    uint32 fragment_id = 1;
    stream_plan.FragmentType fragment_type = 2;
    FragmentDistributionType distribution_type = 3;
    repeated stream_plan.StreamActor actors = 4;
    // Vnode mapping (which should be set in upstream dispatcher) of the fragment.
    common.ParallelUnitMapping vnode_mapping = 5;
  }
  uint32 table_id = 1;
  map<uint32, Fragment> fragments = 2;
  map<uint32, ActorStatus> actor_status = 3;
  repeated uint32 internal_table_ids = 4;
}

// TODO: remove this when dashboard refactored.
message ActorLocation {
  common.WorkerNode node = 1;
  repeated stream_plan.StreamActor actors = 2;
}

message FlushRequest {}

message FlushResponse {
  common.Status status = 1;
}

message ListTableFragmentsRequest {
  repeated uint32 table_ids = 1;
}

message ListTableFragmentsResponse {
  message ActorInfo {
    uint32 id = 1;
    stream_plan.StreamNode node = 2;
    repeated stream_plan.Dispatcher dispatcher = 3;
  }
  message FragmentInfo {
    uint32 id = 1;
    repeated ActorInfo actors = 4;
  }
  message TableFragmentInfo {
    repeated FragmentInfo fragments = 1;
  }
  map<uint32, TableFragmentInfo> table_fragments = 1;
}

service StreamManagerService {
  rpc Flush(FlushRequest) returns (FlushResponse);
  rpc ListTableFragments(ListTableFragmentsRequest) returns (ListTableFragmentsResponse);
}

// Below for cluster service.

message AddWorkerNodeRequest {
  common.WorkerType worker_type = 1;
  common.HostAddress host = 2;
  uint64 worker_node_parallelism = 3;
}

message AddWorkerNodeResponse {
  common.Status status = 1;
  common.WorkerNode node = 2;
}

message ActivateWorkerNodeRequest {
  common.HostAddress host = 1;
}

message ActivateWorkerNodeResponse {
  common.Status status = 1;
}

message DeleteWorkerNodeRequest {
  common.HostAddress host = 1;
}

message DeleteWorkerNodeResponse {
  common.Status status = 1;
}

message ListAllNodesRequest {
  common.WorkerType worker_type = 1;
  // Whether to include nodes still starting
  bool include_starting_nodes = 2;
}

message ListAllNodesResponse {
  common.Status status = 1;
  repeated common.WorkerNode nodes = 2;
}

service ClusterService {
  rpc AddWorkerNode(AddWorkerNodeRequest) returns (AddWorkerNodeResponse);
  rpc ActivateWorkerNode(ActivateWorkerNodeRequest) returns (ActivateWorkerNodeResponse);
  rpc DeleteWorkerNode(DeleteWorkerNodeRequest) returns (DeleteWorkerNodeResponse);
  rpc ListAllNodes(ListAllNodesRequest) returns (ListAllNodesResponse);
}

// Below for notification service.
message SubscribeRequest {
  common.WorkerType worker_type = 1;
  common.HostAddress host = 2;
}

message MetaSnapshot {
  repeated common.WorkerNode nodes = 1;
  repeated catalog.Database database = 2;
  repeated catalog.Schema schema = 3;
  repeated catalog.Source source = 4;
  repeated catalog.Sink sink = 5;
  repeated catalog.Table table = 6;
  repeated user.UserInfo users = 7;
}

message SubscribeResponse {
  enum Operation {
    UNSPECIFIED = 0;
    ADD = 1;
    DELETE = 2;
    UPDATE = 3;
    SNAPSHOT = 4;
  }
  common.Status status = 1;
  Operation operation = 2;
  uint64 version = 3;
  oneof info {
    common.WorkerNode node = 4;
    catalog.Database database = 5;
    catalog.Schema schema = 6;
    catalog.Table table = 7;
    catalog.Source source = 8;
    catalog.Sink sink = 12;
    user.UserInfo user = 11;
    MetaSnapshot snapshot = 9;
    hummock.HummockSnapshot hummock_snapshot = 10;
  }
}

service NotificationService {
  rpc Subscribe(SubscribeRequest) returns (stream SubscribeResponse);
}

message MetaLeaderInfo {
  string node_address = 1;
  uint64 lease_id = 2;
}

message MetaLeaseInfo {
  MetaLeaderInfo leader = 1;
  uint64 lease_register_time = 2;
  uint64 lease_expire_time = 3;
}

message PauseRequest {}

message PauseResponse {}

message ResumeRequest {}

message ResumeResponse {}

message GetClusterInfoRequest {}

message GetClusterInfoResponse {
  repeated common.WorkerNode worker_nodes = 1;
  repeated TableFragments table_fragments = 2;
  message KafkaSplit {
    string topic = 1;
    int32 partition = 2;
    string group_id = 3;
  }
  message MultipleSplitItem {
    oneof split {
      // only consider KafkaSplit for now
      KafkaSplit kafka_split = 1;
    }
  }
  message MultipleSplits {
    repeated MultipleSplitItem items = 1;
  }
  map<uint32, MultipleSplits> actor_splits = 3;
  map<uint32, catalog.StreamSourceInfo> stream_source_infos = 4;
}

service ScaleService {
  // TODO(Kexiang): delete them when config change interface is finished
  rpc Pause(PauseRequest) returns (PauseResponse);
  rpc Resume(ResumeRequest) returns (ResumeResponse);
  rpc GetClusterInfo(GetClusterInfoRequest) returns (GetClusterInfoResponse);
}
